## 主题: BNTO 先验实验设计与度量方案（针对 `ray_trainer.py` 基线）

### 1) 目标与假设
- **目标**: 证明引入“熵预算 + 线性 KL + 负优势放大”的控制后，在相同步数/算力下，训练更稳、奖励更高、熵不早崩、探索更聚焦。
- **主要假设**:
  - H1: BNTO 控制使得 `wH_per_token` 在早期显著高于基线且不崩塌；中后期能按预算缓慢下降；
  - H2: 在相同 KL 目标下，BNTO 的 `reward_sum_mean`/Pass@k 曲线更快上升，AUC 更大；
  - H3: BNTO 降低高协方差 token 的比例/强度（说明“优势-对数概率”的耦合被抑制，更新更稳）。

### 2) 对比组 (最小改动、同超参)
- C0: PPO/GRPO 基线（不启用 dual-game；保持现有 `ray_trainer.py` 行为）
- C1: BNTO-最小（启用 λ/β 控制；`w = p(1-p)|A|`；γ=1.2；预算按当前实现 `decay_target`）
- C2: BNTO-平台衰减（同 C1，但预算使用 EMA 平台触发衰减）
- C3: BNTO-w(含KL门控)（对比 `w = p(1-p)|A|*|logp-logp0|`，评估对探索的抑制影响）

种子: 3 或 5；步数: 1~3k（小规模）、10~20k（中规模）；数据: GSM8K 子集/可用快速基准。

### 3) 关键开关
- γ（负优势放大）: {0.8, 1.2, 1.5}
- ρ（B0 系数）: {0.3, 0.45, 0.6}
- α_λ（lambda_lr）: {0.02, 0.05}
- β 学习率（beta_lr）: {0.01, 0.02}；目标 KL: {0.02, 0.05}

### 4) 指标定义与粒度
- 统一使用 token 级掩码聚合，确保变长序列可比；常规只上传**聚合标量**，周期性抽样直方分布（降低 I/O）。

1) 熵（token 级）
  - `H = -(exp(logp) * logp)`；
  - `H_mean = mean(H[M])`，`H_p50/p90`（可选周期性直方分布）。

2) 协方差（token 级）
  - `cov_t = (A - mean(A[M])) * (logp - mean(logp[M]))`；
  - `cov_mean = mean(cov_t[M])`；可增 `corr = cov / (std(A[M])*std(logp[M])+eps)` 便于跨任务对比。
  - 说明: 选择**per-token** 是因为策略更新是 token 粒度，序列聚合会淹没高影响 token 的信号。

3) W*H（token 级 → 聚合）
  - `w = p(1-p)*abs(A)`（C3 对比含 KL 门控）；`wH = w * H`；
  - `wH_sum = sum(wH[M])`；`wH_per_token = wH_sum / sum(M)`；
  - `usage_ratio = wH_sum / max(B, 1e-8)` 用于观察预算使用；
  - 同步记录 `B_token = B / sum(M)`、`lambda`、`beta`、`KL_per_token`。

4) Reward（序列级 → 批均值）
  - `R_seq = sum(token_level_rewards * M, dim=-1)`；
  - `reward_sum_mean = mean(R_seq)`；可加 AUC（训练横轴归一）。

5) 训练稳定性与效率（辅助）
  - `clipfrac_upper/lower`、`grad_norm`（若已存在）
  - 通过 `AUC(reward)`、`time-to-X`（达到阈值的步数）衡量样本效率。

### 5) 判定准则
- 早期 10% 步内: `H_mean`、`wH_per_token` 显著高于 C0，且无剧烈振荡（方差更小）；
- 全程: `usage_ratio` 随步数缓慢下行，`lambda`/`beta` 曲线平滑；
- 结果: `reward_sum_mean`/Pass@k 曲线 AUC 更大、终值更高，跨种子方差降低；
- 稳定性: `cov_mean` 与 `corr` 降低或更集中（分布尾部变短），`clipfrac` 下降或更稳。

### 6) 采样与日志策略
- 每步上传标量: `analysis/entropy_per_token_mean`、`analysis/covariance_mean`、`analysis/wH_sum`、`analysis/wH_per_token`、`analysis/reward_sum_mean`、`dual_game/{lambda,beta,B_token,KL_per_token,usage_ratio}`。
- 每 N 步（如 200）抽样分布: `H`、`cov_t` 直方图（或 p50/p90）；写到文件或 logger 的 hist 接口（若有）。

### 7) 实验执行步骤（先验）
1. 保持 `ray_trainer.py` 仅做指标上传最小改动；
2. 固定优化器/学习率/rollout 超参与数据划分，设置 seeds=[0,1,2]；
3. 运行 C0/C1/C2/C3；
4. 统一收集曲线：`H_mean`、`wH_per_token`、`usage_ratio`、`lambda`、`beta`、`KL_per_token`、`reward_sum_mean`；
5. 计算 `AUC(reward)`、终值均值±方差；进行配对检验（可选）；
6. 可选: GPU 时间等成本指标，展示性价比曲线。

### 8) 关于“协方差是否需要 per-token?”
- 训练更新是 token 级别，`wH` 和剪裁也在 token 级，因此**推荐 per-token 计算并做掩码平均**作为主指标；
- 为可比性与低开销，日志记录“聚合标量”（mean/quantile）；
- 仅在诊断期，周期性抽样分布/分位数，或计算归一化相关系数 `corr`，避免尺度影响。

### 9) 预期图表
- `H_mean` vs steps；`wH_per_token` / `B_token` vs steps；`usage_ratio` vs steps；
- `lambda`、`beta`、`KL_per_token` vs steps；
- `reward_sum_mean`/Pass@k vs steps（含 AUC 附表）；
- `cov_mean`/`corr` vs steps（含直方图快照）。

### 10) 最小改动提示
- 仅在 `compute_advantage(...)` 之后插入一段张量计算与 `metrics.update`；保留你当前 trainer 的控制逻辑不变。

